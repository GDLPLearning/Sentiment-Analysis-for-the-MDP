{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will show how tweets will be extracted using the twitter API. This requires a developer research account. It is necessary to review the requirements to have access to this account. he advantage of this is that as a developer research account you can access any tweet in the historical Twitter database. Other types of accounts only allow access for a few days (see `elevated case` section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import some libraries together with the `bearer` file containing the `bearer token` of the research account. This is important for establishing the connection with the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tweepy # To consume Twitter's API\n",
    "import bearer # To get the bearer token\n",
    "import pandas as pd # To handle data from the Twitter API\n",
    "import datetime # To handle dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create the connection. With `search_all` we will ask the API to search the tweet history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "tweepyclient=tweepy.Client(bearer_token=bearer.bearer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the search we will create a function that allows to fix the text, removing links. The rest of the dataset is not altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_text(texto):\n",
    "\n",
    "  '''receives the text of the tweet and delivers it in a way that does not affect the csv structure'''\n",
    "\n",
    "  words = texto.split()\n",
    "  tweet = ''\n",
    "  for w in words:\n",
    "    try:\n",
    "      w_ini = w[:4]\n",
    "    except:\n",
    "      w_ini = w\n",
    "    # filtering to remove links from the tweet\n",
    "    if w_ini != 'http': tweet+= f' {w}'\n",
    "\n",
    "  return tweet\n",
    "  # create the connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select the year for which we want to download the tweets. For this, we will vary the variable `anio` between 2019 and 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating a list of dictionaries with all the dates for which the search will be made\n",
    "anio = 2022         # modify the year for which tweets are to be generated\n",
    "\n",
    "start_date = datetime.date(anio,1,1)\n",
    "dates_list = []\n",
    "while start_date < datetime.date(anio,12,31):\n",
    "  end_date = start_date + datetime.timedelta(days=10)\n",
    "  if end_date > datetime.date(anio,12,31):\n",
    "    end_date = datetime.date(anio,12,31)\n",
    "\n",
    "  dates_dict = {\n",
    "      'start_date':start_date,\n",
    "      'end_date':end_date\n",
    "  }\n",
    "  dates_list.append(dates_dict)\n",
    "  start_date = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will proceed to create the data with the downloaded tweets. The tweets will be downloaded by day and by filtered word. For the year 2019 the word chosen will be `Medellin`. Once the data understanding is done and the categories are selected (see `Data preparation` notebook), we will proceed to perform a new search for the years 2019 to 2022 with those words. In this way, the tweets (including those of 2019 that were previously together) will be separated in files differentiated by day and category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates_list:\n",
    "  \n",
    "  try:\n",
    "\n",
    "    print(f'corriendo {date[\"start_date\"]}')\n",
    "    # extracting the dates from the list of dates\n",
    "    # date = dates_list[0]\n",
    "\n",
    "    # query parameters\n",
    "    query = 'medellin tecnologia -is:retweet lang:es' \n",
    "\n",
    "    start_year = str(date['start_date'])[:4]\n",
    "    start_month = str(date['start_date'])[5:7]\n",
    "    start_day = str(date['start_date'])[-2:]\n",
    "    start_time = f'{start_year}-{start_month}-{start_day}T00:00:00Z'\n",
    "\n",
    "    end_year = str(date['end_date'])[:4]\n",
    "    end_month = str(date['end_date'])[5:7]\n",
    "    end_day = str(date['end_date'])[-2:]\n",
    "    end_time = f'{end_year}-{end_month}-{end_day}T00:00:00Z'\n",
    "\n",
    "    # get the tweets\n",
    "\n",
    "    # text and id came by default\n",
    "    tweets = tweepyclient.search_all_tweets(\n",
    "        query=query, \n",
    "        tweet_fields=['created_at',\n",
    "                      'public_metrics',\n",
    "                      'geo',\n",
    "                      'conversation_id',\n",
    "                      'author_id'], \n",
    "        user_fields = ['username'],\n",
    "        place_fields = ['name',\n",
    "                        'full_name'],\n",
    "        expansions=['author_id',\n",
    "                    'geo.place_id'],\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        max_results=500)\n",
    "\n",
    "    # saving the data into a DF\n",
    "\n",
    "    # Get users list from the includes object\n",
    "    users = {u[\"id\"]: u for u in tweets.includes['users']}\n",
    "\n",
    "    # Get list of places from includes object\n",
    "    places = {p[\"id\"]: p for p in tweets.includes['places']}\n",
    "\n",
    "    # creating the dic to fill with the tweets that we fetch\n",
    "    diccionario = {\n",
    "        'full_text':[],\n",
    "        'user':[],\n",
    "        'location':[],\n",
    "        'date':[],\n",
    "        'tweet_id':[],\n",
    "        'number_rt':[],\n",
    "        'number_likes':[],\n",
    "        'number_reply':[],\n",
    "        'conversation_id':[]\n",
    "    }\n",
    "\n",
    "    diccionario_to_fill = diccionario.copy()\n",
    "\n",
    "    # saving the tweets data into a dictionary\n",
    "    for tweet in tweets.data:\n",
    "\n",
    "      diccionario_to_fill['full_text'].append(tweet.text)\n",
    "\n",
    "      if users[tweet.author_id]:\n",
    "        user = users[tweet.author_id]\n",
    "        diccionario_to_fill['user'].append(user.username)\n",
    "      else:\n",
    "        diccionario_to_fill['user'].append('')\n",
    "\n",
    "      diccionario_to_fill['location'].append(str(list(places.values())[0]))\n",
    "      diccionario_to_fill['date'].append(tweet.created_at)\n",
    "      diccionario_to_fill['tweet_id'].append(tweet.id)\n",
    "      diccionario_to_fill['number_rt'].append(tweet.public_metrics['retweet_count'])\n",
    "      diccionario_to_fill['number_likes'].append(tweet.public_metrics['like_count'])\n",
    "      diccionario_to_fill['number_reply'].append(tweet.public_metrics['reply_count'])\n",
    "      diccionario_to_fill['conversation_id'].append(tweet.conversation_id)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(diccionario_to_fill)\n",
    "\n",
    "    # limpiando la columna con el texto del tweet\n",
    "    df['full_text'] = df['full_text'].apply(fixing_text)\n",
    "\n",
    "    st_day = start_time[8:10]\n",
    "    st_month = start_time[5:7]\n",
    "    st_year = start_time[:4]\n",
    "    et_day = end_time[8:10]\n",
    "    et_month = end_time[5:7]\n",
    "    et_year = end_time[:4]\n",
    "    df.to_csv(f'data/research/tweets_{st_day}{st_month}{st_year}_{et_day}{et_month}{et_year}.csv',index = False)\n",
    "\n",
    "    for i in range(0,100000000):\n",
    "      i =+ i\n",
    "      if i > 23423:\n",
    "        i =- 10\n",
    "      else:\n",
    "        i =+ 15\n",
    "  except:\n",
    "    print('#Error')\n",
    "    print(date['start_date'])\n",
    "    print(date['end_date'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
